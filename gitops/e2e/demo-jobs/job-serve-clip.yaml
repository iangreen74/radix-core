apiVersion: batch/v1
kind: Job
metadata:
  name: demo-serve-clip
  namespace: demo
  labels:
    app.kubernetes.io/name: radix-demo
    app.kubernetes.io/component: inference
    app.kubernetes.io/job-type: serve-clip
  annotations:
    app.kubernetes.io/job-type: "serve-clip"
    gpu.mem.gi: "24"
    ml.batch_size: "8"
    scheduler.radix.ai/tenant: "production"
    scheduler.radix.ai/priority: "medium"
spec:
  template:
    metadata:
      labels:
        app.kubernetes.io/name: radix-demo
        app.kubernetes.io/component: inference
        app.kubernetes.io/job-type: serve-clip
      annotations:
        app.kubernetes.io/job-type: "serve-clip"
        gpu.mem.gi: "24"
        ml.batch_size: "8"
        scheduler.radix.ai/tenant: "production"
    spec:
      restartPolicy: Never
      containers:
      - name: server
        image: busybox:1.36
        imagePullPolicy: IfNotPresent
        command: ["sh", "-c"]
        securityContext:
          runAsNonRoot: true
          runAsUser: 1000
          allowPrivilegeEscalation: false
          readOnlyRootFilesystem: false
          seccompProfile:
            type: RuntimeDefault
        args:
        - |
          echo "Starting CLIP inference service simulation..."
          echo "Job Type: serve-clip"
          echo "GPU Memory Required: 24GB"
          echo "Batch Size: 8"
          echo "Tenant: production"
          echo "Simulating inference serving for 20 seconds..."
          sleep 20
          echo "Inference service completed successfully!"
        resources:
          requests:
            cpu: "1"
            memory: "2Gi"
            nvidia.com/gpu: "1"
          limits:
            cpu: "2"
            memory: "4Gi"
            nvidia.com/gpu: "1"
        env:
        - name: JOB_TYPE
          value: "serve-clip"
        - name: GPU_MEMORY_GB
          value: "24"
        - name: BATCH_SIZE
          value: "8"
